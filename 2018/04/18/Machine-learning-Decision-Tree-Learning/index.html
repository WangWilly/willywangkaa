<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-tw,en,default">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/willywangkaa/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/willywangkaa/css/main.css?v=6.0.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/willywangkaa/images/apple-touch-icon-next_cat.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/willywangkaa/images/favicon-32x32-next_cat.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/willywangkaa/images/favicon-16x16-next_cat.png?v=6.0.4">


  <link rel="mask-icon" href="/willywangkaa/images/logo_cat.svg?v=6.0.4" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '6.0.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  
  <meta name="keywords" content="Decision Tree Lrearning,Data Mining,Machine Learning," />


<meta name="description" content="Decision Tree Learning 決策樹學習  簡介  最受歡迎的歸納推理演算法( inductive inference algorithm )。 廣泛且實務的方法。 對於干擾值( Noise )相當敏感。 可用來學習如何以聯集( Disjunctive )表示限制集( Constraints )。 ( Concept Learning 以交集( Conjunctive )表示 )">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine learning - Decision Tree Learning">
<meta property="og:url" content="http://wangwilly.github.io/willywangkaa/2018/04/18/Machine-learning-Decision-Tree-Learning/index.html">
<meta property="og:site_name" content="WillyWangkaa">
<meta property="og:description" content="Decision Tree Learning 決策樹學習  簡介  最受歡迎的歸納推理演算法( inductive inference algorithm )。 廣泛且實務的方法。 對於干擾值( Noise )相當敏感。 可用來學習如何以聯集( Disjunctive )表示限制集( Constraints )。 ( Concept Learning 以交集( Conjunctive )表示 )">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="http://wangwilly.github.io/willywangkaa/images/decisiontreelearning_training%20example.png">
<meta property="og:image" content="http://wangwilly.github.io/willywangkaa/images/decisiontreelearning_exampletree.png">
<meta property="og:image" content="http://wangwilly.github.io/willywangkaa/images/decisiontreelearning_builddecisiontree_algorithm.png">
<meta property="og:image" content="http://wangwilly.github.io/willywangkaa/images/decisiontreelearning_howtochoosenode.png">
<meta property="og:image" content="http://wangwilly.github.io/willywangkaa/images/decisiontreelearning_trainningdataform.png">
<meta property="og:image" content="http://wangwilly.github.io/willywangkaa/images/decisiontreelearning_choosenode.png">
<meta property="og:image" content="http://wangwilly.github.io/willywangkaa/images/decisiontreelearning_finaldecisiontree.png">
<meta property="og:image" content="http://wangwilly.github.io/willywangkaa/images/decisiontreelearning_entropygraph.png">
<meta property="og:image" content="http://wangwilly.github.io/willywangkaa/images/decisiontreelearning_generalentropygraph.png">
<meta property="og:image" content="http://wangwilly.github.io/willywangkaa/images/decisiontreelearning_overfitting1.png">
<meta property="og:image" content="http://wangwilly.github.io/willywangkaa/images/decisiontreelearning_overfitting2.png">
<meta property="og:image" content="http://wangwilly.github.io/willywangkaa/images/decisiontreelearning_subtreeraising.png">
<meta property="article:published_time" content="2018-04-18T04:06:00.000Z">
<meta property="article:modified_time" content="2019-02-08T04:34:35.309Z">
<meta property="article:author" content="Wang Yu Li">
<meta property="article:tag" content="Decision Tree Lrearning">
<meta property="article:tag" content="Data Mining">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://wangwilly.github.io/willywangkaa/images/decisiontreelearning_training%20example.png">






  <link rel="canonical" href="http://WangWilly.github.io/willywangkaa/2018/04/18/Machine-learning-Decision-Tree-Learning/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>
  <title>Machine learning - Decision Tree Learning | WillyWangkaa</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-tw">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/willywangkaa/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">WillyWangkaa</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">The higher up, the greater the fall.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/willywangkaa/%20" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/willywangkaa/archives/%20" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/willywangkaa/categories/%20" rel="section">
            <i class="menu-item-icon fa fa-fw fa-th"></i> <br />Categories</a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/willywangkaa/tags/%20" rel="section">
            <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />Tags</a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />Search</a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  
    <div class="reading-progress-bar"></div>
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://WangWilly.github.io/willywangkaa/willywangkaa/2018/04/18/Machine-learning-Decision-Tree-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/willywangkaa/images/avatar_me.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WillyWangkaa">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Machine learning - Decision Tree Learning</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-18T12:06:00+08:00">2018-04-18</time>
            

            
            
              
                
              
            

            
              
              <span class="post-meta-divider">|</span>
              

              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-02-08T12:34:35+08:00">2019-02-08</time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/willywangkaa/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/willywangkaa/2018/04/18/Machine-learning-Decision-Tree-Learning/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/04/18/Machine-learning-Decision-Tree-Learning/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="decision-tree-learning-決策樹學習">Decision Tree Learning 決策樹學習</h1>
<hr>
<h2 id="簡介">簡介</h2>
<ul>
<li><p>最受歡迎的<strong>歸納推理演算法( inductive inference algorithm )</strong>。</p></li>
<li><p>廣泛且實務的方法。</p></li>
<li><p><strong>對於干擾值( Noise )相當敏感</strong>。</p></li>
<li><p>可用來學習如何以<strong>聯集( Disjunctive )</strong>表示<strong>限制集( Constraints )</strong>。 ( <a href>Concept Learning</a> 以<strong>交集( Conjunctive )</strong>表示 )</p></li>
<li><p>呈現的方式相當簡單。</p></li>
<li><p>樹狀結構( Tree Structure )、若則表示式( If-Then rules )</p></li>
</ul>
<h2 id="ex.-play-tennis"><span class="math inline">\(Ex.\)</span> Play Tennis</h2>
<h3 id="範例訓練資料-training-example">範例訓練資料( Training Example )</h3>
<figure>
<img src="\willywangkaa\images\decisiontreelearning_training%20example.png" alt="decisiontreelearning_training example"><figcaption aria-hidden="true">decisiontreelearning_training example</figcaption>
</figure>
<h3 id="決策樹-decision-tree">決策樹( Decision Tree )</h3>
<figure>
<img src="\willywangkaa\images\decisiontreelearning_exampletree.png" alt="decisiontreelearning_exampletree"><figcaption aria-hidden="true">decisiontreelearning_exampletree</figcaption>
</figure>
<h2 id="決策樹-decision-tree-的介紹">決策樹( Decision Tree )的介紹</h2>
<h3 id="決策樹表示法-decision-tree-representation">決策樹表示法( Decision Tree Representation )</h3>
<ul>
<li>每個<strong>內節點 Internal node ( 包括根結點 Root node )</strong>代表對一個<strong>環境狀態( Attribute )</strong>檢驗。</li>
<li>而<strong>分支( Branch )</strong>出來的意義我們可以視為是該<strong>環境狀態( Attribute )</strong>的一種可能<strong>值( Attribute value )</strong>。</li>
<li>每個<strong>葉節點 Leaf node </strong>給予一個適當的<strong>分類結果( Classification )</strong>。</li>
<li>我們將每個<strong>案例( Instances )</strong>分類到一個離散的<strong>類別( Categories )</strong>之中。</li>
<li>藉由<strong>決策樹</strong>由<strong>根結點</strong>至<strong>葉節點</strong>找到該<strong>類別</strong>。</li>
</ul>
<h3 id="決策樹引導的假說-hypotheses">決策樹引導的假說( Hypotheses )</h3>
<ul>
<li>先 AND 再 OR ( 原文：Disjunctions (OR’s) of conjunctions (AND’s) )。</li>
<li>經由根結點往葉節點走可視為是一種對於該環境狀態限制的交集( Conjunction of constraints on attributes )。</li>
<li>而連上兄弟節點( Sibling )的兩個邊( Edges )可視為是一種對於該環境狀態限制的聯集( Separate branches are disjunctions )。</li>
<li><span class="math inline">\(Ex \; ( Cont. )\)</span>
<div style="text-align: center">
(Outlook=Sunny and Humidity=Normal)
</div>
<div style="text-align: center">
or
</div>
<div style="text-align: center">
(Outlook=Overcast)
</div>
<div style="text-align: center">
or
</div>
<div style="text-align: center">
(Outlook=Rain and Wind=Weak)
</div></li>
</ul>
<h4 id="注意">注意！</h4>
<ul>
<li>每個用來訓練的<strong>案例 ( Instances )</strong>都必須要以「因素-結果」( Attribute - value pairs )的方式給予訓練。</li>
<li>目標訓練函式 ( Target function )的值域是<strong>離散的值 ( Discrete value )</strong>。</li>
<li>這種方法最後呈現的<strong>假說 ( Hypotheses )</strong>有可能是一些<strong>環境狀態限制( Constraints on attributes )</strong>的聯集( Disjunctive )。</li>
<li>極有可能會被不乾淨的資料( Noise )擾亂了學習。</li>
<li>應用於：</li>
<li>醫療或是設備的診斷。</li>
<li>信用額度分析( 銀行 )。</li>
</ul>
<h3 id="決策樹的種類">決策樹的種類</h3>
<p>世界上有許多有特殊的<strong>決策樹演算法( decision-tree algorithms )</strong>，比較著名的有： - ID3 (Iterative Dichotomiser 3) - C4.5, C5.0 (successor of ID3) - CART (Classification And Regression Tree) - CHAID (CHi-squared Automatic Interaction Detector). - MARS: extends decision trees to handle numerical data better.</p>
<h4 id="注意-1">注意</h4>
<ul>
<li>ID3 is the algorithm discussed in textbook.( 在書本中有更詳細的介紹 )</li>
<li>Simple, but representative. ( 簡單但representative? )</li>
<li>Source code publicly available. ( 程式碼是開放的 )</li>
</ul>
<h3 id="id3演算法">ID3演算法</h3>
<ul>
<li><strong>概述</strong>：Top-down, greedy search through <strong>space of possible decision trees</strong>. ( 在所有可以出現的決策樹中用貪心法由上而下找到較佳的那棵樹。 )<br> ID3在建構決策樹過程中，以資訊獲利(Information Gain)為準則，並選擇最大的資訊獲利值作為分類屬性。這個算法是建立在奧卡姆剃刀的基礎上：越是小型的決策樹越優於大的決策樹。儘管如此，該算法也不是總是生成最小的樹形結構，而是一個啟發式算法。另外，C4.5算法是ID3的升級版。<br><br></li>
<li>Decision trees represent hypotheses, so this is a search through hypothesis space. ( 決策樹亦也代表是一種假說，所以這個演算法也可以說是在所有的假說中找到一個較佳的假說。 )</li>
<li><strong>那個演算法該如何起手呢？</strong> 決定甚麼<strong>環境因素( Attribute )</strong>應該放在<strong>根結點( Root node )</strong>？</li>
<li>接著由上而下( Top-down )的建構決策樹，對每個後繼的節點( Successive node )使出一樣的決策手段選出該節點應該置入何種<strong>環境因素( Attribute )</strong>。</li>
<li><strong>注意！</strong>千萬不要由下往上參考之前選過的值，因為我們以貪心法則，所以目前的最佳解決不可能出現在之前選過的<strong>環境因素( Attribute )</strong>之中，或是受其干擾。 ( Never backtracks to reconsider earlier choices. )</li>
<li>同上述，在每次的選擇之中，由於我們認知這種情況適用<strong>貪心法( Greedy Method )</strong>，所以我們每次<strong>環境因素( Attribute )</strong>的選擇都朝向我們最後最佳的<strong>假說</strong>靠近。</li>
</ul>
<h4 id="虛擬碼-pseudo-code">虛擬碼( Pseudo Code )</h4>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 使用屬性計算與之相關的樣本熵值</span><br><span class="line">2. 選取其中熵值最小的屬性(資訊獲利最大)</span><br><span class="line">3. 生成包含該屬性的節點</span><br><span class="line">4. 遞迴直到終止</span><br></pre></td></tr></table></figure>
<figure>
<img src="\willywangkaa\images\decisiontreelearning_builddecisiontree_algorithm.png" alt="decisiontreelearning_builddecisiontree_algorithm"><figcaption aria-hidden="true">decisiontreelearning_builddecisiontree_algorithm</figcaption>
</figure>
<figure>
<img src="\willywangkaa\images\decisiontreelearning_howtochoosenode.png" alt="decisiontreelearning_howtochoosenode"><figcaption aria-hidden="true">decisiontreelearning_howtochoosenode</figcaption>
</figure>
<p><strong>＜討論＞</strong> ID3演算法的終極目標，就是要將決策樹中每個節點都擺上最優的<strong>環境因素( Attributes )</strong>。<br> <span class="math inline">\(Question.\)</span><br> 到底以甚麼條件決定甚麼因素要擺放於哪個節點？ <span class="math inline">\(Answer.\)</span> 資訊獎賞 or 資訊獲利( Information gain )。 - 資訊獲利( Information gain ) 統計該價值以檢視該環境因素置於何處來分類我們的資料，我們使用<strong>熵( entropy 又稱"亂度" )</strong>來定義這邊的<strong>資訊獲利( Information gain )</strong>。( 原文：Statistical quantity measuring how well an attribute classifies the data. Use entropy to define information gain. )</p>
<h3 id="id3-和-c4.5---information-gain-資訊獲利-與-gain-ratio">ID3 和 C4.5 - Information gain ( 資訊獲利 ) 與 Gain ratio</h3>
<h4 id="定義">定義</h4>
<p>關心其中一個環境因素( Attribute )<span class="math inline">\(A\)</span> 的資訊獲利( Information gain )我們標記為 <span class="math inline">\(Gain( S, A )\)</span>，且我們關心的目標樣本群體為 <span class="math inline">\(S\)</span>，其中： <span class="math display">\[Gain( S, A ) = Entropy( S ) - \sum_{ v \in Values(A) } ( \frac{S_v}{S}Entropy(S_v) )\]</span> - <span class="math inline">\(v\)</span> ranges over values of <span class="math inline">\(A\)</span> - <span class="math inline">\(S_v\)</span>: members of <span class="math inline">\(S\)</span> with <span class="math inline">\(A = v\)</span> - <span class="math inline">\(1^{st}\)</span> term: the entropy of <span class="math inline">\(S\)</span> - <span class="math inline">\(2^{nd}\)</span> term: expected value of entropy after partitioning with <span class="math inline">\(A\)</span></p>
<h4 id="example-playtennies">Example： PlayTennies</h4>
<ul>
<li><p>四個環境變因</p>
<ul>
<li>Outlook = {Sunny, Overcast, Rain}</li>
</ul></li>
<li><p>Temperature = {Hot, Mild, Cool}</p>
<ul>
<li>Humidity = {High, Normal} Wind = {Weak, Strong}</li>
</ul></li>
<li><p>欲看討的結果 - <strong>開心</strong>或是<strong>不開心</strong>( Target Attributes - Binary )</p>
<ul>
<li>PlayTennis = {Yes, No}</li>
</ul></li>
<li><p>今天有14組訓練資料</p></li>
<li><p>9筆的結果是開心的 ( Positive )</p></li>
<li><p>5筆的結果是不開心的( Negative )</p></li>
<li><p>訓練資料表</p></li>
</ul>
<figure>
<img src="\willywangkaa\images\decisiontreelearning_trainningdataform.png" alt="decisiontreelearning_trainningdataform"><figcaption aria-hidden="true">decisiontreelearning_trainningdataform</figcaption>
</figure>
<h5 id="step-1.-計算整體的亂度-entropy">Step 1. 計算整體的亂度( Entropy )</h5>
<p><span class="math inline">\(N_\oplus = 9, N_\ominus = 5, N_{Total} = 14\)</span> <span class="math inline">\(Entropy( S ) = -\frac{9}{14} \cdot \lg (\frac{9}{14}) - \frac{5}{14} \cdot \lg ( \frac{5}{14} ) = 0.940\)</span></p>
<h5 id="step2.-不斷計算資訊獲利-找亂度比較低attribute的-選擇最大值當作根結點">Step2. 不斷計算<strong>資訊獲利( 找亂度比較低attribute的 )</strong>，選擇最大值當作根結點</h5>
<ul>
<li>Outlook</li>
<li>Outlook = Sunny</li>
</ul>
<p><span class="math display">\[N_\oplus = 2, N_\ominus = 3, N_{Sunny} = 5\]</span></p>
<p><span class="math display">\[Entropy(S_{Sunny}) = -(\frac{2}{5})\cdot \log_2(\frac{2}{5}) - (\frac{3}{5}) \cdot \log_2(\frac{3}{5}) = 0.971\]</span></p>
<ul>
<li>Outlook = Overcast <span class="math display">\[N_\oplus = 4, N_\ominus = 0, N_{Overcast} = 4\]</span> <span class="math display">\[Entropy(S_{Overcast}) = -(\frac{4}{4})\cdot \log_2(\frac{4}{4}) - (\frac{0}{4}) \cdot \log_2(\frac{0}{4}) = 0.0\]</span></li>
<li>Outlook = Rain <span class="math display">\[N_\oplus = 3, N_\ominus = 2, N_{Rain} = 5\]</span> <span class="math display">\[Entropy(S_{Rain}) = -(\frac{3}{5})\cdot \log_2(\frac{3}{5}) - (\frac{2}{5}) \cdot \log_2(\frac{2}{5}) = 0.971\]</span></li>
<li>計算環境因素的 Outlook 之資訊獲利</li>
</ul>
<p><span class="math display">\[Gain(S, Outlook) = Entropy(S) - (N_{Sunny} / N_{total}) * Entropy(S_{Sunny})\]</span></p>
<p><span class="math display">\[ - (N_{Overcast} / N_{total}) * Entropy(S_{Overcast})\]</span></p>
<p><span class="math display">\[ - (N_{Rain} / N_{total} ) * Entropy(S_{Rain})\]</span></p>
<p><span class="math display">\[\Rightarrow 0.940 - (5/14) \cdot 0.971 - (4/14) \cdot 0.00 - (5/14) \cdot 0.971 = 0.246\]</span></p>
<ul>
<li>Temperature</li>
<li>Repeat process over { Hot, Mild, Cool } <span class="math display">\[ Gain( S, Temperature ) = 0.029 \]</span></li>
<li>Humidity</li>
<li>Repeat process over { High, Normal } <span class="math display">\[ Gain( S, Humidity ) = 0.151 \]</span></li>
<li>Wind</li>
<li>Repeat process over { Weak, Strong } <span class="math display">\[ Gain( S, Wind ) = 0.048 \]</span></li>
</ul>
<p>再來，我們要找到最佳的資訊獲利( Information gain )，其中：</p>
<p><span class="math display">\[Gain(S, Outlook) = 0.246\]</span></p>
<p><span class="math display">\[ Gain( S, Temperature ) = 0.029 \]</span></p>
<p><span class="math display">\[ Gain( S, Humidity ) = 0.151 \]</span></p>
<p><span class="math display">\[ Gain( S, Wind ) = 0.048 \]</span></p>
<p>從亂度的點看來，似乎Outlook的亂度最低( 與宇亂度相減後剩餘比較多資訊獲利 )，所以我們選擇<strong>Outlook</strong>作為我們根結點( root node )，如下圖：</p>
<figure>
<img src="\willywangkaa\images\decisiontreelearning_choosenode.png" alt="decisiontreelearning_choosenode"><figcaption aria-hidden="true">decisiontreelearning_choosenode</figcaption>
</figure>
<p>選擇了Outlook做為決策樹的根結點後，緊接著，我們可以將三種不同的Outlook作為分支，其中特別的是，Overcast狀態之中( 上圖中間綠色部分 )，全部皆為開心狀態( Positive outcome )，所以可以直接決定Overcast輸出為開心( Positive )。</p>
<h5 id="step-2.-conti.---選擇下一個節點-子樹的根結點">Step 2. Conti. - 選擇下一個節點( 子樹的根結點 )</h5>
<p>( 從何子節點開始建子樹？ I don't know yet. )</p>
<ul>
<li>Same steps as earlier but only examples sorted to the node are used in Gain computations.( 無法理解 )</li>
<li>選一個點( 隨機？ )繼續建子樹</li>
<li>Outlook = Sunny</li>
</ul>
<p><span class="math display">\[Gain(S_{Sunny}, Humidity) = 0.97 - (3/5) \cdot 0 - (2/5) \cdot 0 = 0.97 bits\]</span></p>
<p><span class="math display">\[Gain(S_{Sunny}, Temperature) = 0.97 - (2/5) \cdot 0 - (2/5) \cdot 1 - (1/5) \cdot 0 = 0.57 bits\]</span></p>
<p><span class="math display">\[Gain(S_{Sunny}, Wind) = 0.97 - (2/5) \cdot 1 - (3/5) \cdot 0.918 = 0.019 bits\]</span></p>
<p>由上式可以看出來<strong>Humidity</strong>的亂度最小，所以選擇之為此子樹的根。</p>
<h5 id="final-decision-tree">Final Decision Tree</h5>
<figure>
<img src="\willywangkaa\images\decisiontreelearning_finaldecisiontree.png" alt="decisiontreelearning_finaldecisiontree"><figcaption aria-hidden="true">decisiontreelearning_finaldecisiontree</figcaption>
</figure>
<h1 id="熵亂度-entropy">熵、亂度 (Entropy)</h1>
<hr>
<h2 id="介紹">介紹</h2>
<p>在資訊理論中，熵被用來衡量一個<strong>隨機變數出現的期望值</strong>(機率與統計)。它代表了在被接收之前，訊號傳輸過程中損失的資訊量，又被稱為資訊熵。熵是對<strong>不確定性</strong>的測量。在資訊界，熵<strong>越高</strong>則能<strong>傳輸越多的資訊</strong>( 資訊越多意味著有<strong>更多的可能性</strong> )，熵<strong>越低</strong>則意味著<strong>傳輸的資訊越少</strong>( 資訊越少意味著有<strong>更少的可能性</strong> )。<br><br> 如果有一枚理想的硬幣，其出現正面和反面的機會相等，則拋硬幣事件的熵等於其能夠達到的最大值。我們無法知道下一個硬幣拋擲的結果是什麼，因此每一次拋硬幣都是不可預測的。( 越是不可預測的結果 <span class="math inline">\(\rightarrow\)</span> 亂度越大，而這種結果，正是造成人類<strong>選擇障礙</strong>的原因，所以我們希望熵越低越好，我們可以立即做出判斷 )</p>
<h3 id="ex1."><span class="math inline">\(Ex1.\)</span></h3>
<p>使用一枚<strong>正常硬幣</strong>進行拋擲，這個事件的熵是一位元，若進行n次獨立實驗，則熵為<span class="math inline">\(n\)</span>，因為可以用長度為 <span class="math inline">\(n\)</span> 的位元流表示。但是如果一枚硬幣的兩面完全相同，那個這個系列拋硬幣事件的熵等於<strong>零</strong>，因為結果<strong>能被準確預測</strong>。</p>
<h3 id="ex2."><span class="math inline">\(Ex2.\)</span></h3>
<p><span class="math inline">\(Let \; y \; be \; a \; Boolean \; function, and \; let \; P \; denote \; Probability.\)</span> What is the most pure (亂度低) probability distribution? <span class="math display">\[P(y = 0) = 1, P(y = 1) = 0\]</span> <span class="math display">\[P(y = 0) = 0, P(y = 1) = 1\]</span></p>
<p>What is the most impure (亂度高) probability distribution? <span class="math display">\[P(y = 0) = 0.5, P(y = 1) = 0.5\]</span> 意同於最大的亂度。</p>
<h2 id="定義-1">定義</h2>
<p>首先，我們可以先從簡單的看討當目前的結果最多只有兩種情況，如拋硬幣，最多只有正面或是反面，下圖<span class="math inline">\(x\)</span>軸<span class="math inline">\(P_\oplus\)</span>代表擲出正面的機率函數，而<span class="math inline">\(y\)</span>軸則是對應的熵值，而<span class="math inline">\(P_\ominus\)</span>的機率軸則是會隨著<span class="math inline">\(P_\oplus\)</span>下降而上升( 兩者互補 )，但是對應到的熵值會一樣大。</p>
<figure>
<img src="\willywangkaa\images\decisiontreelearning_entropygraph.png" alt="decisiontreelearning_entropygraph"><figcaption aria-hidden="true">decisiontreelearning_entropygraph</figcaption>
</figure>
<p><span class="math inline">\(S\)</span> is a sample of training examples( 隨機變量 ). 當今天的結果只有正與反 ( 與硬幣一樣 )時，觀察目前的<strong>隨機變量</strong> - 我們令： - <span class="math inline">\(P_\oplus\)</span> ( 就目前隨機變數產生的機率 ) is the portion of the positive examples ( 正面 ) in <span class="math inline">\(S\)</span>. - <span class="math inline">\(P_\ominus\)</span> ( 就目前隨機變數產生的機率 ) is the portion of the negative examples ( 反面 ) in <span class="math inline">\(S\)</span>. Entropy ( 熵 ) measures the impurity ( 亂度 ) of <span class="math inline">\(S\)</span>.</p>
<p>我們先定義熵值 ( Entropy ) 如下： <span class="math display">\[ Entropy( S ) = E( I( S ) ) = E(- \ln ( P ( S ) ) ) \]</span> 其中，<span class="math inline">\(E\)</span>為<strong>期望函數</strong>，<span class="math inline">\(I( S )\)</span>是 <span class="math inline">\(S\)</span> 的<strong>資訊量</strong>（又稱為<a href="https://zh.wikipedia.org/wiki/%E8%87%AA%E4%BF%A1%E6%81%AF" target="_blank" rel="noopener">資訊本體</a>），<span class="math inline">\(I( S )\)</span>也是一個<strong>隨機變數</strong>。 所以在取硬幣的樣本( <span class="math inline">\(S\)</span> )完後，我們可以將熵值寫成：</p>
<p><span class="math display">\[ Entropy( S ) = \sum_{i = 1}^{2} P(S_i)I(S_i)\]</span></p>
<p><span class="math display">\[ \Rightarrow -\sum_{i = 1}^{2} P(S_i)\log_{2} P(S_i) \]</span></p>
<p><span class="math display">\[ \Rightarrow  -P_{\oplus}\log_2 P_{\oplus} - P_{\ominus}\log_2 P_{\ominus}\]</span></p>
<p><strong>＜Note＞</strong> <span class="math display">\[\sum_{i = 1}^N P_i = 1 \; and \;  0 \leq P_i \leq 1 \]</span></p>
<h3 id="推廣至一般式"><strong>推廣至一般式</strong></h3>
<figure>
<img src="\willywangkaa\images\decisiontreelearning_generalentropygraph.png" alt="decisiontreelearning_generalentropygraph"><figcaption aria-hidden="true">decisiontreelearning_generalentropygraph</figcaption>
</figure>
<p>當取自有限的樣本時，熵的公式可以表示為： <span class="math display">\[H(X) = \sum _{i} P(x_i) \, I(x_i)=-\sum_i P(x_i)\log _b P(x_i)\]</span></p>
<p>在這裏 <span class="math inline">\(b\)</span> 通常是<span class="math inline">\(2\)</span>,自然常數 <span class="math inline">\(e\)</span>，或是<span class="math inline">\(10\)</span>。當<span class="math inline">\(b = 2\)</span>，熵的單位是<span class="math inline">\(bit\)</span>；當<span class="math inline">\(b = e\)</span>，熵的單位是<span class="math inline">\(nat\)</span>；而當<span class="math inline">\(b = 10\)</span>,熵的單位是<span class="math inline">\(Hart\)</span>。</p>
<p><strong>＜Note＞</strong> 定義當<span class="math inline">\(P_i = 0\)</span>時，對於一些 <span class="math inline">\(i\)</span> 值，對應的被加數 <span class="math inline">\(0 \log_b 0\)</span> 的值將會是 <span class="math inline">\(0\)</span>，這與極限一致。 <span class="math display">\[ \Rightarrow \lim_{p\to0+} ( p\log p ) = 0 \]</span></p>
<h1 id="cart-classification-and-regression-tree">CART (Classification and Regression Tree)</h1>
<hr>
<p>見 <a href="http://mropengate.blogspot.tw/2015/06/ai-ch13-2-decision-tree.html" target="_blank" rel="noopener">Mr' opengate - AI - Ch14 機器學習(2), 決策樹 Decision Tree</a></p>
<h1 id="決策樹學習的常見問題">決策樹學習的常見問題</h1>
<hr>
<h2 id="避免過度適配資料-prevent-overfitting">避免過度適配資料( Prevent Overfitting )</h2>
<p>首先，相較於很冗長的樹，在機器學習中其實比較偏向於比較矮的樹，然而，為何？我們可以由<a href="https://zh.wikipedia.org/wiki/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80" target="_blank" rel="noopener">Occam’s Razor ( 奧坎剃刀 )</a>得知，若有兩個假說同時都能解釋該現象，我們偏向於比較沒那麼嚴個的假說( 可以表達比較廣的概念 )。<br>過度配適是指模型對於範例的過度訓練，導致模型記住的不是訓練資料的一般特性，反而是訓練資料的局部特性。對測試樣本的分類將會變得很不精確。</p>
<p><strong>＜注意＞</strong>通常過度適配發生在訓練範例含有雜訊和離異值時，但當訓練數據沒有雜訊時，過度適配也有可能發生，特別是當訓練範例的數量太少，使得某一些屬性「恰巧」可以很好地分割目前的訓練範例，但卻與實際的狀況並無太多關係。</p>
<figure>
<img src="\willywangkaa\images\decisiontreelearning_overfitting1.png" alt="decisiontreelearning_overfitting1"><figcaption aria-hidden="true">decisiontreelearning_overfitting1</figcaption>
</figure>
<figure>
<img src="\willywangkaa\images\decisiontreelearning_overfitting2.png" alt="decisiontreelearning_overfitting2"><figcaption aria-hidden="true">decisiontreelearning_overfitting2</figcaption>
</figure>
<h2 id="解決方案修剪決策樹移除不可信賴的分支">解決方案：修剪決策樹移除不可信賴的分支</h2>
<ul>
<li>事前修剪 (Prepruning) : 透過決策樹不再增長的方式來達到修剪的目的。選擇一個合適的臨界值往往很困難。</li>
<li>事後修剪 (Postpruning) :</li>
<li>子樹置換 (Subtree Replacement)：選擇某個子樹，並用單個樹葉來置換它。</li>
<li>子樹提升 (Subtree Raising)：</li>
</ul>
<figure>
<img src="\willywangkaa\images\decisiontreelearning_subtreeraising.png" alt="decisiontreelearning_subtreeraising"><figcaption aria-hidden="true">decisiontreelearning_subtreeraising</figcaption>
</figure>
<h2 id="合併連續值屬性">合併連續值屬性</h2>
<p>透過動態地定義新的離散值屬性來實現，即先把連續值屬性的值域分割為離散的區間集合，或設定門檻值以進行二分法。</p>
<h2 id="屬性選擇指標的其他度量標準">屬性選擇指標的其他度量標準</h2>
<ul>
<li>訊息獲利 : 趨向於包含多個值的屬性</li>
<li>獲利比率 : 會產生不平均的分割，也就是分割的一邊會非常小於另一邊</li>
<li>吉尼係數 : 傾向於包含多個值的屬性，當類別個數很多時會有困難，傾向那些會導致平衡切割並且兩邊均為純粹的測試</li>
</ul>
<p><strong>＜尚有其他的度量標準，也都各有利弊＞</strong></p>
<h1 id="例題">例題</h1>
<hr>
<ul>
<li>A data set has 4 Boolean variables. What is the maximum number of leaves in a decision tree? <span class="math inline">\(2^4\)</span></li>
<li>To each leaf in the decision, the number of corresponding rule is <strong>1</strong></li>
<li>If a decision tree achieves 100% accuracy on the training set, then it will also get 100% accuracy on the test set? <strong>No</strong></li>
<li>Using information gain to pick attributes, decision tree learning can be considered <strong>A* search algorithm.</strong> <strong>No</strong></li>
<li>A decision tree can describe any Boolean function? <strong>Yes</strong></li>
</ul>
<h1 id="補充">補充</h1>
<hr>
<ul>
<li>C4.5 演算法
<ul>
<li>C4.5演算法利用屬性的獲利比率(Gain Ratio)克服問題，獲利比率是資訊獲利正規化後的結果。求算某屬性A的獲利比率時除資訊獲 利外，尚需計算該屬性的分割資訊值(Split Information) : SplitInfoA(S)=∑t∈T|Sj||S|×log2|Sj||S|</li>
</ul></li>
<li>C4.5的改善：對連續屬性的處理
<ul>
<li>改善ID3傾向選擇擁有許多不同數值但不具意義的屬性：之所以使用獲利比率(Gain Ratio)，是因為ID3演算法所使用的資訊獲利會傾向選擇擁有許多不同數值的屬性，例如：若依學生學號(獨一無二的屬性)進行分割，會產生出許多分支，且每一個分支都是很單一的結果，其資訊獲利會最大。但這個屬性對於建立決策樹是沒有意義的。</li>
</ul></li>
<li>C5.0 演算法
<ul>
<li>C5.0 是 C4.5的商業改進版，可應用於海量資料集合上之分類。主要在執行準確度和記憶體耗用方面做了改進。因其採用Boosting方式來提高模型準確率，且佔用系統資源與記憶體較少，所以計算速度較快。其所使用的演算法沒有被公開。</li>
</ul></li>
<li>C5.0 的優點：
<ul>
<li>C5.0模型在面對遺漏值時非常穩定。</li>
<li>C5.0模型不需要很長的訓練次數。</li>
<li>C5.0模型比較其他類型的模型易於理解。</li>
<li>C5.0的增強技術提高分類的精度。</li>
</ul></li>
</ul>
<h1 id="參考">參考</h1>
<hr>
<p><a href="http://mropengate.blogspot.tw/2015/06/ai-ch13-2-decision-tree.html" target="_blank" rel="noopener">Mr' opengate - AI - Ch14 機器學習(2), 決策樹 Decision Tree</a></p>
<p><a href="https://zh.wikipedia.org/wiki/%E7%86%B5_%E4%BF%A1%E6%81%AF%E8%AE%BA" target="_blank" rel="noopener">Wiki - 熵 - 資訊理論</a></p>
<p><a href="https://zh.wikipedia.org/wiki/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80" target="_blank" rel="noopener">奧坎剃刀</a></p>

      
    </div>

    
      


    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/willywangkaa/tags/Decision-Tree-Lrearning/" rel="tag"># Decision Tree Lrearning</a>
          
            <a href="/willywangkaa/tags/Data-Mining/" rel="tag"># Data Mining</a>
          
            <a href="/willywangkaa/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/willywangkaa/2018/03/21/Algorithm-%E5%AD%97%E4%B8%B2%E5%90%8D%E8%A9%9E%E7%B0%A1%E4%BB%8B/" rel="next" title="Algorithm - 字串名詞簡介">
                <i class="fa fa-chevron-left"></i> Algorithm - 字串名詞簡介
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/willywangkaa/2018/04/20/Machine-learning-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%9C%B0%E5%9C%96/" rel="prev" title="Machine learning - 機器學習地圖">
                Machine learning - 機器學習地圖 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a target="_blank" rel="noopener" href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/willywangkaa/images/avatar_me.gif"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/willywangkaa/archives/%20%7C%7C%20archive">
                
                    <span class="site-state-item-count">61</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/willywangkaa/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">10</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/willywangkaa/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">76</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://typora.io/" title="Typora" target="_blank">Typora</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.geeksforgeeks.org/" title="GeeksforGeeks" target="_blank">GeeksforGeeks</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.codecogs.com/latex/eqneditor.php" title="Online LaTeX Equation Editor" target="_blank">Online LaTeX Equation Editor</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://allem40306.github.io/blog/" title="CodingJack" target="_blank">CodingJack</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#decision-tree-learning-%E6%B1%BA%E7%AD%96%E6%A8%B9%E5%AD%B8%E7%BF%92"><span class="nav-number">1.</span> <span class="nav-text">Decision Tree Learning 決策樹學習</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B0%A1%E4%BB%8B"><span class="nav-number">1.1.</span> <span class="nav-text">簡介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ex.-play-tennis"><span class="nav-number">1.2.</span> <span class="nav-text">\(Ex.\) Play Tennis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AF%84%E4%BE%8B%E8%A8%93%E7%B7%B4%E8%B3%87%E6%96%99-training-example"><span class="nav-number">1.2.1.</span> <span class="nav-text">範例訓練資料( Training Example )</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree"><span class="nav-number">1.2.2.</span> <span class="nav-text">決策樹( Decision Tree )</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E7%9A%84%E4%BB%8B%E7%B4%B9"><span class="nav-number">1.3.</span> <span class="nav-text">決策樹( Decision Tree )的介紹</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%BA%E7%AD%96%E6%A8%B9%E8%A1%A8%E7%A4%BA%E6%B3%95-decision-tree-representation"><span class="nav-number">1.3.1.</span> <span class="nav-text">決策樹表示法( Decision Tree Representation )</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%BA%E7%AD%96%E6%A8%B9%E5%BC%95%E5%B0%8E%E7%9A%84%E5%81%87%E8%AA%AA-hypotheses"><span class="nav-number">1.3.2.</span> <span class="nav-text">決策樹引導的假說( Hypotheses )</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">注意！</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%BA%E7%AD%96%E6%A8%B9%E7%9A%84%E7%A8%AE%E9%A1%9E"><span class="nav-number">1.3.3.</span> <span class="nav-text">決策樹的種類</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F-1"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">注意</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#id3%E6%BC%94%E7%AE%97%E6%B3%95"><span class="nav-number">1.3.4.</span> <span class="nav-text">ID3演算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%99%9B%E6%93%AC%E7%A2%BC-pseudo-code"><span class="nav-number">1.3.4.1.</span> <span class="nav-text">虛擬碼( Pseudo Code )</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#id3-%E5%92%8C-c4.5---information-gain-%E8%B3%87%E8%A8%8A%E7%8D%B2%E5%88%A9-%E8%88%87-gain-ratio"><span class="nav-number">1.3.5.</span> <span class="nav-text">ID3 和 C4.5 - Information gain ( 資訊獲利 ) 與 Gain ratio</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E7%BE%A9"><span class="nav-number">1.3.5.1.</span> <span class="nav-text">定義</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#example-playtennies"><span class="nav-number">1.3.5.2.</span> <span class="nav-text">Example： PlayTennies</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#step-1.-%E8%A8%88%E7%AE%97%E6%95%B4%E9%AB%94%E7%9A%84%E4%BA%82%E5%BA%A6-entropy"><span class="nav-number">1.3.5.2.1.</span> <span class="nav-text">Step 1. 計算整體的亂度( Entropy )</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#step2.-%E4%B8%8D%E6%96%B7%E8%A8%88%E7%AE%97%E8%B3%87%E8%A8%8A%E7%8D%B2%E5%88%A9-%E6%89%BE%E4%BA%82%E5%BA%A6%E6%AF%94%E8%BC%83%E4%BD%8Eattribute%E7%9A%84-%E9%81%B8%E6%93%87%E6%9C%80%E5%A4%A7%E5%80%BC%E7%95%B6%E4%BD%9C%E6%A0%B9%E7%B5%90%E9%BB%9E"><span class="nav-number">1.3.5.2.2.</span> <span class="nav-text">Step2. 不斷計算資訊獲利( 找亂度比較低attribute的 )，選擇最大值當作根結點</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#step-2.-conti.---%E9%81%B8%E6%93%87%E4%B8%8B%E4%B8%80%E5%80%8B%E7%AF%80%E9%BB%9E-%E5%AD%90%E6%A8%B9%E7%9A%84%E6%A0%B9%E7%B5%90%E9%BB%9E"><span class="nav-number">1.3.5.2.3.</span> <span class="nav-text">Step 2. Conti. - 選擇下一個節點( 子樹的根結點 )</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#final-decision-tree"><span class="nav-number">1.3.5.2.4.</span> <span class="nav-text">Final Decision Tree</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%86%B5%E4%BA%82%E5%BA%A6-entropy"><span class="nav-number">2.</span> <span class="nav-text">熵、亂度 (Entropy)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%8B%E7%B4%B9"><span class="nav-number">2.1.</span> <span class="nav-text">介紹</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ex1."><span class="nav-number">2.1.1.</span> <span class="nav-text">\(Ex1.\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ex2."><span class="nav-number">2.1.2.</span> <span class="nav-text">\(Ex2.\)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E7%BE%A9-1"><span class="nav-number">2.2.</span> <span class="nav-text">定義</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E5%BB%A3%E8%87%B3%E4%B8%80%E8%88%AC%E5%BC%8F"><span class="nav-number">2.2.1.</span> <span class="nav-text">推廣至一般式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#cart-classification-and-regression-tree"><span class="nav-number">3.</span> <span class="nav-text">CART (Classification and Regression Tree)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B1%BA%E7%AD%96%E6%A8%B9%E5%AD%B8%E7%BF%92%E7%9A%84%E5%B8%B8%EF%A8%8A%E5%95%8F%E9%A1%8C"><span class="nav-number">4.</span> <span class="nav-text">決策樹學習的常見問題</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%81%BF%E5%85%8D%E9%81%8E%EF%A8%81%E9%81%A9%E9%85%8D%E8%B3%87%EF%A6%BE-prevent-overfitting"><span class="nav-number">4.1.</span> <span class="nav-text">避免過度適配資料( Prevent Overfitting )</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E6%B1%BA%E6%96%B9%E6%A1%88%E4%BF%AE%E5%89%AA%E6%B1%BA%E7%AD%96%E6%A8%B9%E7%A7%BB%E9%99%A4%EF%A5%A7%E5%8F%AF%E4%BF%A1%E8%B3%B4%E7%9A%84%E5%88%86%E6%94%AF"><span class="nav-number">4.2.</span> <span class="nav-text">解決方案：修剪決策樹移除不可信賴的分支</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%88%E4%BD%B5%EF%A6%9A%E7%BA%8C%E5%80%BC%E5%B1%AC%E6%80%A7"><span class="nav-number">4.3.</span> <span class="nav-text">合併連續值屬性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%AC%E6%80%A7%E9%81%B8%E6%93%87%E6%8C%87%E6%A8%99%E7%9A%84%E5%85%B6%E4%BB%96%EF%A8%81%EF%A5%BE%E6%A8%99%E6%BA%96"><span class="nav-number">4.4.</span> <span class="nav-text">屬性選擇指標的其他度量標準</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BE%8B%E9%A1%8C"><span class="nav-number">5.</span> <span class="nav-text">例題</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A3%9C%E5%85%85"><span class="nav-number">6.</span> <span class="nav-text">補充</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%83%E8%80%83"><span class="nav-number">7.</span> <span class="nav-text">參考</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Yu Li</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Pisces</a> v6.0.4</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>
























  



  
  
    <script type="text/javascript" src="/willywangkaa/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/willywangkaa/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/willywangkaa/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/willywangkaa/lib/reading_progress/reading_progress.js"></script>
  


  


  <script type="text/javascript" src="/willywangkaa/js/src/utils.js?v=6.0.4"></script>

  <script type="text/javascript" src="/willywangkaa/js/src/motion.js?v=6.0.4"></script>



  
  


  <script type="text/javascript" src="/willywangkaa/js/src/affix.js?v=6.0.4"></script>

  <script type="text/javascript" src="/willywangkaa/js/src/schemes/pisces.js?v=6.0.4"></script>



  
  <script type="text/javascript" src="/willywangkaa/js/src/scrollspy.js?v=6.0.4"></script>
<script type="text/javascript" src="/willywangkaa/js/src/post-details.js?v=6.0.4"></script>



  


  <script type="text/javascript" src="/willywangkaa/js/src/bootstrap.js?v=6.0.4"></script>



  

  
    <script id="dsq-count-scr" src="https://https-wangwilly-github-io-blog.disqus.com/count.js" async></script>
  

  
    <script type="text/javascript">
      var disqus_config = function () {
        this.page.url = 'http://wangwilly.github.io/willywangkaa/2018/04/18/Machine-learning-Decision-Tree-Learning/';
        this.page.identifier = '2018/04/18/Machine-learning-Decision-Tree-Learning/';
        this.page.title = 'Machine learning - Decision Tree Learning';
      };
      function loadComments () {
        var d = document, s = d.createElement('script');
        s.src = 'https://https-wangwilly-github-io-blog.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      }
      
        loadComments();
      
    </script>
  





	





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/willywangkaa/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  
  
  
  <script src="/willywangkaa/lib/bookmark/bookmark.min.js?v=1.0"></script>
  <script type="text/javascript">
  
    bookmark.scrollToMark('auto', "#more");
  
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</body>
</html>
